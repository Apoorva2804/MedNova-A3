{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06ca57e3-d583-4ce2-ae10-908205ab4a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face login successful.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    login(hf_token)\n",
    "    print(\"Hugging Face login successful.\")\n",
    "else:\n",
    "    raise EnvironmentError(\"HF_TOKEN not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c701d89-86bc-447d-a981-757ff2d59b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56c5a356-adfe-436a-a8b8-ad7d444814fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 12:54:17,110 [INFO] Scraping PubMed for query: diabetes treatment\n",
      "2025-04-16 12:54:58,678 [INFO] Saved 50 articles to medical_data/diabetes_treatment_pubmed.json\n",
      "2025-04-16 12:54:58,681 [INFO] Scraping medical terms for letter: a\n",
      "2025-04-16 12:55:00,515 [ERROR] Error scraping terms for letter 'a': 404 Client Error: Not Found for url: https://www.medicinenet.com/script/main/alphaidx.asp?p=a\n",
      "2025-04-16 12:55:00,516 [INFO] Scraping medical terms for letter: b\n",
      "2025-04-16 12:55:01,710 [ERROR] Error scraping terms for letter 'b': 404 Client Error: Not Found for url: https://www.medicinenet.com/script/main/alphaidx.asp?p=b\n",
      "2025-04-16 12:55:01,711 [INFO] Scraping medical terms for letter: c\n",
      "2025-04-16 12:55:02,983 [ERROR] Error scraping terms for letter 'c': 404 Client Error: Not Found for url: https://www.medicinenet.com/script/main/alphaidx.asp?p=c\n",
      "2025-04-16 12:55:02,987 [INFO] Saved 0 medical terms to medical_data/medical_terminology.json\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Medical Data Scraper Class\n",
    "# -------------------------------\n",
    "class MedicalDataScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Medical Research Bot'\n",
    "        }\n",
    "        self.data_dir = 'medical_data'\n",
    "        os.makedirs(self.data_dir, exist_ok=True)\n",
    "\n",
    "    def scrape_pubmed(self, query, max_results=100):\n",
    "        \"\"\"Scrape article titles and abstracts from PubMed.\"\"\"\n",
    "        logging.info(f\"Scraping PubMed for query: {query}\")\n",
    "        \n",
    "        base_url = \"https://pubmed.ncbi.nlm.nih.gov\"\n",
    "        search_url = f\"{base_url}/?term={query.replace(' ', '+')}\"\n",
    "        \n",
    "        articles = []\n",
    "        page = 1\n",
    "        results_count = 0\n",
    "        filename = os.path.join(self.data_dir, f\"{query.replace(' ', '_')}_pubmed.json\")\n",
    "\n",
    "        while results_count < max_results:\n",
    "            try:\n",
    "                response = requests.get(f\"{search_url}&page={page}\", headers=self.headers)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                article_elements = soup.select('.docsum-content')\n",
    "                if not article_elements:\n",
    "                    break\n",
    "\n",
    "                for article in article_elements:\n",
    "                    if results_count >= max_results:\n",
    "                        break\n",
    "                    \n",
    "                    title_element = article.select_one('.docsum-title')\n",
    "                    title = title_element.text.strip() if title_element else \"No title\"\n",
    "                    abstract_link = title_element.get('href') if title_element else None\n",
    "                    abstract_url = base_url + abstract_link if abstract_link else None\n",
    "                    abstract = self._get_abstract(abstract_url) if abstract_url else \"No abstract\"\n",
    "\n",
    "                    articles.append({\n",
    "                        'title': title,\n",
    "                        'abstract': abstract,\n",
    "                        'source': abstract_url or \"N/A\",\n",
    "                        'query': query,\n",
    "                        'scraped_at': time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    })\n",
    "\n",
    "                    results_count += 1\n",
    "\n",
    "                    # Save every 10 results as a backup\n",
    "                    if results_count % 10 == 0:\n",
    "                        with open(filename, 'w') as f:\n",
    "                            json.dump(articles, f, indent=2)\n",
    "\n",
    "                page += 1\n",
    "                time.sleep(2)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error scraping page {page}: {e}\")\n",
    "                break\n",
    "\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(articles, f, indent=2)\n",
    "\n",
    "        logging.info(f\"Saved {len(articles)} articles to {filename}\")\n",
    "        return articles\n",
    "\n",
    "    def _get_abstract(self, url):\n",
    "        \"\"\"Helper to get abstract from article detail page.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            abstract_div = soup.select_one('.abstract-content')\n",
    "            return abstract_div.text.strip() if abstract_div else \"Abstract not available\"\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error fetching abstract from {url}: {e}\")\n",
    "            return \"Error fetching abstract\"\n",
    "\n",
    "    def scrape_medical_dictionary(self, letters=None):\n",
    "        \"\"\"Scrape medical terms and definitions from MedicineNet.\"\"\"\n",
    "        if letters is None:\n",
    "            letters = list('abcdefghijklmnopqrstuvwxyz')\n",
    "\n",
    "        base_url = \"https://www.medicinenet.com/script/main/alphaidx.asp?p=\"\n",
    "        all_terms = {}\n",
    "\n",
    "        for letter in letters:\n",
    "            logging.info(f\"Scraping medical terms for letter: {letter}\")\n",
    "            try:\n",
    "                response = requests.get(f\"{base_url}{letter}\", headers=self.headers)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                term_elements = soup.select('.AZ_results a')\n",
    "                terms = {}\n",
    "\n",
    "                for element in term_elements:\n",
    "                    term = element.text.strip()\n",
    "                    link = element.get('href')\n",
    "                    full_link = link if link.startswith(\"http\") else \"https://www.medicinenet.com\" + link\n",
    "\n",
    "                    if term and full_link:\n",
    "                        definition = self._get_term_definition(full_link)\n",
    "                        terms[term] = {\n",
    "                            'definition': definition,\n",
    "                            'source': full_link,\n",
    "                            'scraped_at': time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                        }\n",
    "\n",
    "                all_terms.update(terms)\n",
    "                time.sleep(2)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error scraping terms for letter '{letter}': {e}\")\n",
    "\n",
    "        filename = os.path.join(self.data_dir, \"medical_terminology.json\")\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(all_terms, f, indent=2)\n",
    "\n",
    "        logging.info(f\"Saved {len(all_terms)} medical terms to {filename}\")\n",
    "        return all_terms\n",
    "\n",
    "    def _get_term_definition(self, url):\n",
    "        \"\"\"Helper to get a term's definition from its page.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            definition_div = soup.select_one('.main-content article')\n",
    "            return definition_div.text.strip() if definition_div else \"Definition not available\"\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error fetching definition from {url}: {e}\")\n",
    "            return \"Error fetching definition\"\n",
    "\n",
    "# -------------------------------\n",
    "# Example Usage \n",
    "# -------------------------------\n",
    "scraper = MedicalDataScraper()\n",
    "articles = scraper.scrape_pubmed(\"diabetes treatment\", max_results=50)\n",
    "terms = scraper.scrape_medical_dictionary(letters=['a', 'b', 'c'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2d654ff-4555-47b6-8c6d-5992aac7b44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 12:55:03,925 [INFO] NumExpr defaulting to 8 threads.\n",
      "[nltk_data] Downloading package wordnet to /Users/aditya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aditya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Imports\n",
    "# -------------------------------\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -------------------------------\n",
    "# Setup NLTK Resources (run once)\n",
    "# -------------------------------\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# -------------------------------\n",
    "# Medical Data Processor Class\n",
    "# -------------------------------\n",
    "class MedicalDataProcessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english')).union({\n",
    "            'patient', 'patients', 'doctor', 'doctors', 'hospital', 'treatment'\n",
    "        })\n",
    "        self.data_dir = 'processed_data'\n",
    "        os.makedirs(self.data_dir, exist_ok=True)\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        \"\"\"Load raw JSON data from file.\"\"\"\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Lowercase, remove punctuation, stop words, and lemmatize.\"\"\"\n",
    "        text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "        words = word_tokenize(text)\n",
    "        processed_words = [\n",
    "            self.lemmatizer.lemmatize(word)\n",
    "            for word in words if word not in self.stop_words\n",
    "        ]\n",
    "        return ' '.join(processed_words)\n",
    "\n",
    "    def process_medical_articles(self, file_path):\n",
    "        \"\"\"Clean titles and abstracts from scraped articles.\"\"\"\n",
    "        articles = self.load_data(file_path)\n",
    "        processed_data = []\n",
    "\n",
    "        for article in articles:\n",
    "            processed_data.append({\n",
    "                'title': self.preprocess_text(article['title']),\n",
    "                'abstract': self.preprocess_text(article['abstract']),\n",
    "                'original_title': article['title'],\n",
    "                'original_abstract': article['abstract']\n",
    "            })\n",
    "\n",
    "        output_file = os.path.join(self.data_dir, os.path.basename(file_path))\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(processed_data, f, indent=2)\n",
    "\n",
    "        print(f\" Processed and saved {len(processed_data)} articles to {output_file}\")\n",
    "        return processed_data\n",
    "\n",
    "    def extract_medical_entities(self, text):\n",
    "        \"\"\"Use regex to find basic disease/drug/symptom patterns.\"\"\"\n",
    "        patterns = {\n",
    "            'diseases': [\n",
    "                r'\\b(?:[A-Z][a-z]+ (?:disease|disorder|syndrome))\\b',\n",
    "                r'\\b(?:[A-Z][a-z]+ (?:cancer|tumor|carcinoma))\\b'\n",
    "            ],\n",
    "            'drugs': [\n",
    "                r'\\b[A-Z][a-z]+(?:in|en)\\b',\n",
    "                r'\\b[A-Z][a-z]+(?:ol|al)\\b',\n",
    "                r'\\b[A-Z][a-z]+ide\\b'\n",
    "            ],\n",
    "            'symptoms': [\n",
    "                r'\\b(?:pain|ache|discomfort|fever|swelling) (?:in|of) [a-z]+\\b',\n",
    "                r'\\b(?:chronic|acute|severe) [a-z]+ (?:pain|inflammation)\\b'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        entities = {'diseases': [], 'drugs': [], 'symptoms': []}\n",
    "        for entity_type, regexes in patterns.items():\n",
    "            for pattern in regexes:\n",
    "                matches = re.findall(pattern, text)\n",
    "                if matches:\n",
    "                    entities[entity_type].extend(matches)\n",
    "\n",
    "        return entities\n",
    "\n",
    "    def prepare_training_data(self, processed_articles, split_ratio=0.2):\n",
    "        \"\"\"Label abstracts, split for training, and save to JSON.\"\"\"\n",
    "        texts = [article['original_abstract'] for article in processed_articles]\n",
    "        processed_texts = [article['abstract'] for article in processed_articles]\n",
    "\n",
    "        # Labeling: 1 if text contains 'treatment' or 'therapy'\n",
    "        labels = [\n",
    "            1 if 'treatment' in text.lower() or 'therapy' in text.lower() else 0\n",
    "            for text in texts\n",
    "        ]\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            processed_texts, labels, test_size=split_ratio, random_state=42\n",
    "        )\n",
    "\n",
    "        train_data = {'texts': X_train, 'labels': y_train}\n",
    "        val_data = {'texts': X_val, 'labels': y_val}\n",
    "\n",
    "        with open(os.path.join(self.data_dir, 'train_data.json'), 'w') as f:\n",
    "            json.dump(train_data, f, indent=2)\n",
    "        with open(os.path.join(self.data_dir, 'val_data.json'), 'w') as f:\n",
    "            json.dump(val_data, f, indent=2)\n",
    "\n",
    "        print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
    "        return train_data, val_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "197d0f82-234e-4097-a520-fc4f953358d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    pipeline\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e659aa6-734f-4f83-bbf8-9b1db95efcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalNLPSystem:\n",
    "    def __init__(self, ehr_path='ehr_data.json'):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.models_dir = 'nlp_models'\n",
    "        os.makedirs(self.models_dir, exist_ok=True)\n",
    "\n",
    "        self.ehr_path = ehr_path\n",
    "        if not os.path.exists(self.ehr_path):\n",
    "            with open(self.ehr_path, 'w') as f:\n",
    "                json.dump({}, f)\n",
    "\n",
    "        self._load_models()\n",
    "\n",
    "    def _load_models(self):\n",
    "        print(\"Loading models...\")\n",
    "        self.stt_pipeline = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=\"openai/whisper-medium\",\n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "\n",
    "        self.ner_tokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "        self.ner_model = AutoModelForTokenClassification.from_pretrained(\"d4data/biomedical-ner-all\").to(self.device)\n",
    "        self.ner_pipeline = pipeline(\n",
    "            \"ner\",\n",
    "            model=self.ner_model,\n",
    "            tokenizer=self.ner_tokenizer,\n",
    "            aggregation_strategy=\"simple\",\n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "\n",
    "        self.summarizer_tokenizer = AutoTokenizer.from_pretrained(\"google/pegasus-pubmed\")\n",
    "        self.summarizer_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/pegasus-pubmed\").to(self.device)\n",
    "\n",
    "    def transcribe_audio(self, audio_path):\n",
    "        print(f\"Transcribing: {audio_path}\")\n",
    "        result = self.stt_pipeline(audio_path)\n",
    "        return result[\"text\"]\n",
    "\n",
    "    def extract_entities(self, text):\n",
    "        print(\"Extracting medical entities...\")\n",
    "        raw_entities = self.ner_pipeline(text)\n",
    "        entities = {}\n",
    "        for item in raw_entities:\n",
    "            label = item['entity_group']\n",
    "            word = item['word']\n",
    "            if label not in entities:\n",
    "                entities[label] = []\n",
    "            entities[label].append(word)\n",
    "        return entities\n",
    "\n",
    "    def summarize_text(self, text):\n",
    "        print(\"Summarizing text...\")\n",
    "        inputs = self.summarizer_tokenizer(\n",
    "            text, return_tensors=\"pt\", truncation=True, max_length=1024\n",
    "        ).to(self.device)\n",
    "        summary_ids = self.summarizer_model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=150,\n",
    "            min_length=50,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        return self.summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    def update_ehr(self, patient_id, transcript, summary, entities):\n",
    "        with open(self.ehr_path, 'r') as f:\n",
    "            ehr_data = json.load(f)\n",
    "\n",
    "        if patient_id not in ehr_data:\n",
    "            ehr_data[patient_id] = {\n",
    "                \"records\": []\n",
    "            }\n",
    "\n",
    "        ehr_data[patient_id][\"records\"].append({\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"transcript\": transcript,\n",
    "            \"summary\": summary,\n",
    "            \"entities\": entities\n",
    "        })\n",
    "\n",
    "        with open(self.ehr_path, 'w') as f:\n",
    "            json.dump(ehr_data, f, indent=2)\n",
    "\n",
    "        print(f\"Updated EHR for patient {patient_id}.\")\n",
    "\n",
    "    def process_patient_audio(self, patient_id, audio_path):\n",
    "        transcript = self.transcribe_audio(audio_path)\n",
    "        summary = self.summarize_text(transcript)\n",
    "        entities = self.extract_entities(transcript)\n",
    "        self.update_ehr(patient_id, transcript, summary, entities)\n",
    "        return transcript, summary, entities\n",
    "\n",
    "    def chatbot_response(self, patient_id, question):\n",
    "        with open(self.ehr_path, 'r') as f:\n",
    "            ehr_data = json.load(f)\n",
    "\n",
    "        if patient_id not in ehr_data:\n",
    "            return \"No data found for this patient.\"\n",
    "\n",
    "        latest_record = ehr_data[patient_id][\"records\"][-1]\n",
    "        entities = latest_record['entities']\n",
    "\n",
    "        question_lower = question.lower()\n",
    "        response = \"I'm here to help. \"\n",
    "\n",
    "        if \"symptom\" in question_lower:\n",
    "            response += f\"Noted symptoms: {', '.join(entities.get('SYMPTOM', [])) or 'No symptoms recorded.'}\"\n",
    "        elif \"treatment\" in question_lower:\n",
    "            response += f\"Ongoing treatments: {', '.join(entities.get('TREATMENT', [])) or 'No treatments recorded.'}\"\n",
    "        elif \"diagnosis\" in question_lower:\n",
    "            response += f\"Diagnosis details: {', '.join(entities.get('DISEASE', [])) or 'No diagnosis found.'}\"\n",
    "        else:\n",
    "            response += \"You can ask about symptoms, treatments, or diagnosis.\"\n",
    "\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34962587-4a6c-482d-b6d3-683fefe3d694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-pubmed and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting medical entities...\n",
      "\n",
      "ðŸ§¬ Extracted Entities:\n",
      "Dosage: 500mg\n",
      "Medication: am, ##oxicillin\n",
      "Biological_structure: upper respiratory\n",
      "History: hypertension, type 2 diabetes\n",
      "Diagnostic_procedure: vital\n",
      "Clinical_event: follow\n",
      "Lab_value: 2\n",
      "Summarizing text...\n",
      "\n",
      "ðŸ“„ Generated Summary:\n",
      "key clinical messagea 77-year - old man was admitted to the intensive care unit ( icu ) with a 5-day history of dyspnea , cough , and chest pain . on admission , <n> a chest computed tomography ( ct ) scan revealed a large left pulmonary artery aneurysm . <n> oxicillin was administered , and the patient made an uneventful recovery .\n"
     ]
    }
   ],
   "source": [
    "nlp = MedicalNLPSystem()\n",
    "\n",
    "sample_text = \"\"\"\n",
    "The patient was prescribed 500mg of Amoxicillin for an upper respiratory infection. \n",
    "He has a history of hypertension and Type 2 diabetes. \n",
    "Vitals are stable. Follow-up scheduled in 2 weeks.\n",
    "\"\"\"\n",
    "\n",
    "entities = nlp.extract_entities(sample_text)\n",
    "print(\"\\nðŸ§¬ Extracted Entities:\")\n",
    "for k, v in entities.items():\n",
    "    print(f\"{k}: {', '.join(v)}\")\n",
    "\n",
    "summary = nlp.summarize_text(sample_text)\n",
    "print(\"\\nðŸ“„ Generated Summary:\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d8ea103-7ff3-44eb-b4c1-7988a67decca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class EHRIntegration:\n",
    "    def __init__(self):\n",
    "        self.data_dir = \"ehr_data\"\n",
    "        os.makedirs(self.data_dir, exist_ok=True)\n",
    "\n",
    "    def _get_file_path(self, patient_id):\n",
    "        return os.path.join(self.data_dir, f\"{patient_id}.json\")\n",
    "\n",
    "    def _init_patient_record(self, patient_id):\n",
    "        # Create a blank template for new patient\n",
    "        return {\n",
    "            \"patient_id\": patient_id,\n",
    "            \"demographics\": {},\n",
    "            \"medical_history\": {\n",
    "                \"conditions\": [],\n",
    "                \"allergies\": [],\n",
    "            },\n",
    "            \"medications\": [],\n",
    "            \"lab_results\": {},\n",
    "            \"visits\": [],\n",
    "            \"notes\": [],\n",
    "            \"timestamps\": {\n",
    "                \"created_at\": str(datetime.now()),\n",
    "                \"updated_at\": str(datetime.now())\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def fetch_patient_data(self, patient_id, fields=None):\n",
    "        file_path = self._get_file_path(patient_id)\n",
    "        if not os.path.exists(file_path):\n",
    "            return {\"error\": \"Patient not found\"}\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        if fields:\n",
    "            return {k: data.get(k) for k in fields}\n",
    "        return data\n",
    "\n",
    "    def save_patient_data(self, patient_id, data):\n",
    "        file_path = self._get_file_path(patient_id)\n",
    "        data[\"timestamps\"][\"updated_at\"] = str(datetime.now())\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "    def get_or_create_patient(self, patient_id):\n",
    "        file_path = self._get_file_path(patient_id)\n",
    "        if os.path.exists(file_path):\n",
    "            return self.fetch_patient_data(patient_id)\n",
    "        new_data = self._init_patient_record(patient_id)\n",
    "        self.save_patient_data(patient_id, new_data)\n",
    "        return new_data\n",
    "\n",
    "    def update_patient_data(self, patient_id, updates):\n",
    "        current_data = self.get_or_create_patient(patient_id)\n",
    "\n",
    "        for key, value in updates.items():\n",
    "            if key not in current_data:\n",
    "                current_data[key] = value\n",
    "            elif isinstance(current_data[key], list) and isinstance(value, list):\n",
    "                current_data[key].extend(value)\n",
    "            elif isinstance(current_data[key], dict) and isinstance(value, dict):\n",
    "                current_data[key].update(value)\n",
    "            else:\n",
    "                current_data[key] = value\n",
    "\n",
    "        self.save_patient_data(patient_id, current_data)\n",
    "        return {\"status\": \"success\", \"updated\": updates}\n",
    "\n",
    "    def append_note(self, patient_id, note):\n",
    "        data = self.get_or_create_patient(patient_id)\n",
    "        data[\"notes\"].append({\"note\": note, \"timestamp\": str(datetime.now())})\n",
    "        self.save_patient_data(patient_id, data)\n",
    "        return {\"status\": \"note added\"}\n",
    "\n",
    "    def append_visit(self, patient_id, visit_data):\n",
    "        data = self.get_or_create_patient(patient_id)\n",
    "        visit_data[\"timestamp\"] = str(datetime.now())\n",
    "        data[\"visits\"].append(visit_data)\n",
    "        self.save_patient_data(patient_id, data)\n",
    "        return {\"status\": \"visit added\"}\n",
    "\n",
    "    def create_patient_summary(self, patient_id):\n",
    "        data = self.fetch_patient_data(patient_id)\n",
    "        if \"error\" in data:\n",
    "            return data\n",
    "        age = self._calculate_age(data[\"demographics\"].get(\"dob\")) if data[\"demographics\"].get(\"dob\") else \"N/A\"\n",
    "        return {\n",
    "            \"patient_id\": data[\"patient_id\"],\n",
    "            \"age\": age,\n",
    "            \"gender\": data[\"demographics\"].get(\"gender\", \"N/A\"),\n",
    "            \"conditions\": data[\"medical_history\"].get(\"conditions\", []),\n",
    "            \"allergies\": data[\"medical_history\"].get(\"allergies\", []),\n",
    "            \"medications\": data.get(\"medications\", []),\n",
    "            \"recent_notes\": data.get(\"notes\", [])[-3:],\n",
    "            \"last_updated\": data[\"timestamps\"][\"updated_at\"]\n",
    "        }\n",
    "\n",
    "    def _calculate_age(self, dob):\n",
    "        try:\n",
    "            birth_date = datetime.fromisoformat(dob)\n",
    "            today = datetime.now()\n",
    "            return today.year - birth_date.year - ((today.month, today.day) < (birth_date.month, birth_date.day))\n",
    "        except:\n",
    "            return \"Invalid DOB\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d5e1de8-a3af-45cc-84b0-38c14647cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "EHR_FILE = \"ehr_records.json\"\n",
    "AUDIT_LOG = \"chatbot_audit_log.json\"\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# ðŸ”§ Utility EHR Storage\n",
    "# ------------------------------\n",
    "\n",
    "def load_ehr():\n",
    "    if not os.path.exists(EHR_FILE):\n",
    "        return {}\n",
    "    with open(EHR_FILE, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_ehr(data):\n",
    "    with open(EHR_FILE, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "def generate_numeric_id():\n",
    "    return str(random.randint(100000, 999999))\n",
    "\n",
    "def enroll_new_patient(name, age, gender, medical_history=None):\n",
    "    ehr = load_ehr()\n",
    "    patient_id = generate_numeric_id()\n",
    "    while patient_id in ehr:\n",
    "        patient_id = generate_numeric_id()\n",
    "    ehr[patient_id] = {\n",
    "        \"name\": name,\n",
    "        \"age\": age,\n",
    "        \"gender\": gender,\n",
    "        \"medical_history\": medical_history or [],\n",
    "        \"vitals\": [],\n",
    "        \"records\": [],\n",
    "    }\n",
    "    save_ehr(ehr)\n",
    "    return patient_id\n",
    "\n",
    "def update_medical_record(patient_id, note):\n",
    "    ehr = load_ehr()\n",
    "    if patient_id in ehr:\n",
    "        ehr[patient_id][\"records\"].append({\"note\": note, \"timestamp\": str(datetime.now())})\n",
    "        save_ehr(ehr)\n",
    "        return \"âœ… Medical record updated.\"\n",
    "    return \"âŒ Patient ID not found.\"\n",
    "\n",
    "def update_vitals(patient_id, vitals):\n",
    "    ehr = load_ehr()\n",
    "    if patient_id in ehr:\n",
    "        vitals[\"timestamp\"] = str(datetime.now())\n",
    "        ehr[patient_id][\"vitals\"].append(vitals)\n",
    "        save_ehr(ehr)\n",
    "        return \"âœ… Vitals updated.\"\n",
    "    return \"âŒ Patient ID not found.\"\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# ðŸ“„ EHR Wrapper Class\n",
    "# ------------------------------\n",
    "\n",
    "class EHRIntegration:\n",
    "    def fetch_patient_data(self, patient_id):\n",
    "        ehr = load_ehr()\n",
    "        return ehr.get(patient_id, {\"error\": \"Patient not found\"})\n",
    "\n",
    "    def update_patient_data(self, patient_id, updates):\n",
    "        ehr = load_ehr()\n",
    "        if patient_id not in ehr:\n",
    "            return {\"error\": \"Patient not found\"}\n",
    "        for key, value in updates.items():\n",
    "            if isinstance(ehr[patient_id].get(key), list) and isinstance(value, list):\n",
    "                ehr[patient_id][key].extend(value)\n",
    "            else:\n",
    "                ehr[patient_id][key] = value\n",
    "        save_ehr(ehr)\n",
    "        return {\"status\": \"updated\", \"updates\": updates}\n",
    "\n",
    "    def create_patient_summary(self, patient_id):\n",
    "        data = self.fetch_patient_data(patient_id)\n",
    "        if \"error\" in data:\n",
    "            return data\n",
    "        return {\n",
    "            \"summary\": f\"{data['name']}, {data['age']} y/o, {data['gender']}.\",\n",
    "            \"medical_history\": data.get(\"medical_history\", []),\n",
    "            \"vitals\": data.get(\"vitals\", [])[-3:],\n",
    "            \"records\": data.get(\"records\", [])[-3:]\n",
    "        }\n",
    "\n",
    "    def log_audit_event(self, event_type, user_id, patient_id, details):\n",
    "        log_entry = {\n",
    "            \"timestamp\": str(datetime.now()),\n",
    "            \"event\": event_type,\n",
    "            \"user\": user_id,\n",
    "            \"patient_id\": patient_id,\n",
    "            \"details\": details\n",
    "        }\n",
    "        if not os.path.exists(AUDIT_LOG):\n",
    "            logs = []\n",
    "        else:\n",
    "            with open(AUDIT_LOG, 'r') as f:\n",
    "                logs = json.load(f)\n",
    "        logs.append(log_entry)\n",
    "        with open(AUDIT_LOG, 'w') as f:\n",
    "            json.dump(logs, f, indent=2)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Medical Chatbot Core\n",
    "# ------------------------------\n",
    "\n",
    "class MedicalChatbot:\n",
    "    def __init__(self, knowledge_base_path=None):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BioGPT\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\"microsoft/BioGPT\").to(self.device)\n",
    "\n",
    "        self.knowledge_base = {}\n",
    "        if knowledge_base_path and os.path.exists(knowledge_base_path):\n",
    "            with open(knowledge_base_path, 'r') as f:\n",
    "                self.knowledge_base = json.load(f)\n",
    "\n",
    "        self.ehr = EHRIntegration()\n",
    "\n",
    "    def classify_query_intent(self, query):\n",
    "        intents = {\n",
    "            'request_information': ['what is', 'tell me about', 'explain'],\n",
    "            'documentation': ['note', 'record', 'update'],\n",
    "            'medication': ['prescribe', 'drug', 'dosage'],\n",
    "            'diagnosis': ['diagnose', 'symptoms'],\n",
    "        }\n",
    "        query_lower = query.lower()\n",
    "        for intent, keywords in intents.items():\n",
    "            if any(keyword in query_lower for keyword in keywords):\n",
    "                return intent\n",
    "        return 'general_query'\n",
    "\n",
    "    def extract_medical_terms(self, text):\n",
    "        prefixes = ['hyper', 'hypo', 'neuro', 'cardio']\n",
    "        suffixes = ['itis', 'osis', 'oma', 'algia']\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        return list({word for word in words if any(word.startswith(p) for p in prefixes) or any(word.endswith(s) for s in suffixes)})\n",
    "\n",
    "    def process_query(self, query, patient_id=None, user_id=\"system\"):\n",
    "        intent = self.classify_query_intent(query)\n",
    "        terms = self.extract_medical_terms(query)\n",
    "        context = f\"Doctor asked: {query}\\n\\nIntent: {intent}\\n\"\n",
    "\n",
    "        if patient_id:\n",
    "            summary = self.ehr.create_patient_summary(patient_id)\n",
    "            context += f\"Patient Summary:\\n{json.dumps(summary, indent=2)}\\n\"\n",
    "\n",
    "        # Inject KB terms if found\n",
    "        for term in terms:\n",
    "            if term in self.knowledge_base:\n",
    "                context += f\"{term}: {self.knowledge_base[term]}\\n\"\n",
    "\n",
    "        inputs = self.tokenizer(context, return_tensors=\"pt\", truncation=True).to(self.device)\n",
    "        response_ids = self.model.generate(inputs[\"input_ids\"], max_length=150, do_sample=True, top_p=0.9, temperature=0.7)\n",
    "        response = self.tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Auto update and audit\n",
    "        if intent == 'documentation' and patient_id:\n",
    "            update_medical_record(patient_id, query)\n",
    "            response += \"\\n\\nðŸ“ Patient record has been updated.\"\n",
    "            self.ehr.log_audit_event(\"documentation\", user_id, patient_id, query)\n",
    "\n",
    "        elif intent == 'medication' and patient_id:\n",
    "            data = self.ehr.fetch_patient_data(patient_id)\n",
    "            meds = data.get(\"medications\", [])\n",
    "            allergies = data.get(\"medical_history\", {}).get(\"allergies\", [])\n",
    "            response += f\"\\n\\nðŸ’Š Medications: {', '.join(meds)}\"\n",
    "            response += f\"\\nâš ï¸ Allergies: {', '.join(allergies)}\"\n",
    "            self.ehr.log_audit_event(\"medication_check\", user_id, patient_id, f\"Query: {query}\")\n",
    "\n",
    "        else:\n",
    "            self.ehr.log_audit_event(\"chat_query\", user_id, patient_id or \"unknown\", query)\n",
    "\n",
    "        return response, intent, \", \".join(terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3feea68-eeb0-43d5-91ce-e5a116d71017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "class LocalChatBot:\n",
    "    def __init__(self, model_name=\"google/flan-t5-base\"):\n",
    "        print(\"Loading model, please wait...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        self.pipeline = pipeline(\"text2text-generation\", model=self.model, tokenizer=self.tokenizer)\n",
    "\n",
    "    def process_query(self, query, context=\"\"):\n",
    "        prompt = f\"Answer the question based on the context.\\nContext: {context}\\nQuestion: {query}\"\n",
    "        result = self.pipeline(prompt, max_new_tokens=100, do_sample=False)\n",
    "        return result[0][\"generated_text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0ada863-39b5-4880-8dd8-826e0f67b892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model, please wait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot says: headaches, blurred vision, and fatigue\n"
     ]
    }
   ],
   "source": [
    "bot = LocalChatBot()\n",
    "\n",
    "question = \"What are symptoms of high blood pressure?\"\n",
    "context = \"Patient is experiencing headaches, blurred vision, and fatigue.\"\n",
    "\n",
    "response = bot.process_query(question, context)\n",
    "print(\"Bot says:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60fdb9d4-161e-4558-98aa-b2ba7e9845c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import whisper\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "EHR_FILE = \"ehr_data.json\"\n",
    "\n",
    "class MedicalAIAssistant:\n",
    "    def __init__(self):\n",
    "        print(\"Initializing Medical AI Assistant...\")\n",
    "        self.chatbot = LocalChatBot(model_name=\"google/flan-t5-base\")\n",
    "        self.transcriber = whisper.load_model(\"base\")\n",
    "\n",
    "        # Load biomedical NER model\n",
    "        self.ner_pipeline = pipeline(\n",
    "            \"ner\",\n",
    "            model=\"d4data/biomedical-ner-all\",\n",
    "            tokenizer=\"d4data/biomedical-ner-all\",\n",
    "            aggregation_strategy=\"simple\"\n",
    "        )\n",
    "\n",
    "        # Summarization pipeline\n",
    "        self.summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "        os.makedirs(\"logs\", exist_ok=True)\n",
    "        os.makedirs(\"audit_logs\", exist_ok=True)\n",
    "\n",
    "    def load_ehr(self):\n",
    "        if not os.path.exists(EHR_FILE):\n",
    "            return {}\n",
    "        with open(EHR_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def save_ehr(self, data):\n",
    "        with open(EHR_FILE, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "    def generate_numeric_id(self):\n",
    "        return str(random.randint(100000, 999999))\n",
    "\n",
    "    def enroll_new_patient(self, name, age, gender, medical_history=None):\n",
    "        ehr = self.load_ehr()\n",
    "        patient_id = self.generate_numeric_id()\n",
    "        while patient_id in ehr:\n",
    "            patient_id = self.generate_numeric_id()\n",
    "        ehr[patient_id] = {\n",
    "            \"name\": name,\n",
    "            \"age\": age,\n",
    "            \"gender\": gender,\n",
    "            \"medical_history\": medical_history or [],\n",
    "            \"vitals\": [],\n",
    "            \"records\": [],\n",
    "        }\n",
    "        self.save_ehr(ehr)\n",
    "        return patient_id\n",
    "\n",
    "    def update_patient_record(self, patient_id, note=None, vitals=None):\n",
    "        ehr = self.load_ehr()\n",
    "        if patient_id not in ehr:\n",
    "            return {\"error\": \"Patient ID not found\"}\n",
    "        if note:\n",
    "            ehr[patient_id][\"records\"].append({\n",
    "                \"note\": note,\n",
    "                \"timestamp\": str(datetime.now())\n",
    "            })\n",
    "        if vitals:\n",
    "            vitals[\"timestamp\"] = str(datetime.now())\n",
    "            ehr[patient_id][\"vitals\"].append(vitals)\n",
    "        self.save_ehr(ehr)\n",
    "        return {\"status\": \"updated\", \"patient_id\": patient_id}\n",
    "\n",
    "    def view_patient_record(self, patient_id):\n",
    "        ehr = self.load_ehr()\n",
    "        return ehr.get(patient_id, {\"error\": \"Patient not found\"})\n",
    "\n",
    "    def process_doctor_query(self, query, patient_id=None, user_id=None):\n",
    "        if user_id is None:\n",
    "            return {\"error\": \"User ID required\"}\n",
    "\n",
    "        patient_data = self.view_patient_record(patient_id)\n",
    "        context = json.dumps(patient_data, indent=2)\n",
    "\n",
    "        response_text, intent, terms = self.chatbot.process_query(query, context)\n",
    "\n",
    "        log_entry = {\n",
    "            \"timestamp\": str(datetime.now()),\n",
    "            \"user_id\": user_id,\n",
    "            \"patient_id\": patient_id,\n",
    "            \"query\": query,\n",
    "            \"intent\": intent,\n",
    "            \"terms\": terms,\n",
    "            \"response\": response_text\n",
    "        }\n",
    "\n",
    "        log_path = f\"logs/{datetime.now().strftime('%Y%m%d_%H%M%S')}_{user_id}.json\"\n",
    "        with open(log_path, \"w\") as f:\n",
    "            json.dump(log_entry, f, indent=2)\n",
    "\n",
    "        return {\"response\": response_text, \"intent\": intent, \"terms\": terms}\n",
    "\n",
    "    def fetch_patient_data(self, patient_id):\n",
    "        ehr = self.load_ehr()\n",
    "        return ehr.get(patient_id, {\"error\": \"Patient not found\"})\n",
    "\n",
    "    def update_patient_data(self, patient_id, updates):\n",
    "        ehr = self.load_ehr()\n",
    "        if patient_id not in ehr:\n",
    "            return {\"error\": \"Patient ID not found\"}\n",
    "\n",
    "        if \"notes\" in updates:\n",
    "            ehr[patient_id][\"records\"].append({\n",
    "                \"note\": updates[\"notes\"],\n",
    "                \"timestamp\": str(datetime.now())\n",
    "            })\n",
    "\n",
    "        if \"vital_signs\" in updates:\n",
    "            updates[\"vital_signs\"][\"timestamp\"] = str(datetime.now())\n",
    "            ehr[patient_id][\"vitals\"].append(updates[\"vital_signs\"])\n",
    "\n",
    "        if \"medications\" in updates:\n",
    "            ehr[patient_id].setdefault(\"medications\", []).extend(updates[\"medications\"])\n",
    "\n",
    "        if \"lab_results\" in updates:\n",
    "            ehr[patient_id].setdefault(\"lab_results\", {}).update(updates[\"lab_results\"])\n",
    "\n",
    "        self.save_ehr(ehr)\n",
    "        return {\"status\": \"updated\", \"patient_id\": patient_id}\n",
    "\n",
    "    def process_audio_recording(self, audio_file_path, user_id):\n",
    "        print(f\"Processing audio: {audio_file_path}\")\n",
    "\n",
    "        # Step 1: Transcribe audio to text\n",
    "        result = self.transcriber.transcribe(audio_file_path)\n",
    "        transcript = result[\"text\"]\n",
    "\n",
    "        # Step 2: Extract biomedical entities\n",
    "        ner_results = self.ner_pipeline(transcript)\n",
    "        entities = list(set(item[\"word\"] for item in ner_results))\n",
    "\n",
    "        # Step 3: Summarize the transcript\n",
    "        chunks = [transcript[i:i+1000] for i in range(0, len(transcript), 1000)]\n",
    "        summaries = [\n",
    "            self.summarizer(chunk, max_length=100, min_length=25, do_sample=False)[0][\"summary_text\"]\n",
    "            for chunk in chunks\n",
    "        ]\n",
    "        summary = \" \".join(summaries)\n",
    "\n",
    "        # Step 4: Log the audio processing event\n",
    "        audit_log = {\n",
    "            \"timestamp\": str(datetime.now()),\n",
    "            \"event\": \"audio_processing\",\n",
    "            \"user_id\": user_id,\n",
    "            \"file\": audio_file_path\n",
    "        }\n",
    "        with open(f\"audit_logs/audio_{user_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\", \"w\") as f:\n",
    "            json.dump(audit_log, f, indent=2)\n",
    "\n",
    "        return {\n",
    "            \"transcript\": transcript,\n",
    "            \"entities\": entities,\n",
    "            \"summary\": summary\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3f3bd7d-947c-4847-bc48-1458c52eda38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Medical AI Assistant...\n",
      "Loading model, please wait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio: /Users/aditya/Desktop/65yo-diabete.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "Your max_length is set to 100, but your input_length is only 63. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=31)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Transcript ---\n",
      " The patient is a 65 year old male with a history of type 2 diabetes and hypertension. He reports increased fatigue, blood vision and elevated blood glucose levels over the past week. Current medications include metformin and lycinopiril. Recommend checking HBA1C and adjusting medication if necessary.\n",
      "\n",
      "--- Extracted Medical Entities ---\n",
      "['hyper', 'blood vision', '##y', '##ril', 'medication', 'elevated', '65 year old', '##form', 'met', 'blood glucose', 'l', 'male', 'h', '##in', '##cino', 'type 2 diabetes', 'fatigue', '##pi']\n",
      "\n",
      "--- Summary ---\n",
      "The patient is a 65 year old male with a history of type 2 diabetes and hypertension. He reports increased fatigue, blood vision and elevated blood glucose levels over the past week. Current medications include metformin and lycinopiril.\n"
     ]
    }
   ],
   "source": [
    "assistant = MedicalAIAssistant()\n",
    "\n",
    "audio_path = \"/Users/aditya/Desktop/65yo-diabete.wav\"  # Use your actual audio filename\n",
    "user_id = \"doctor_123\"  # Replace with your user ID if needed\n",
    "\n",
    "result = assistant.process_audio_recording(audio_path, user_id)\n",
    "\n",
    "print(\"\\n--- Transcript ---\")\n",
    "print(result[\"transcript\"])\n",
    "\n",
    "print(\"\\n--- Extracted Medical Entities ---\")\n",
    "print(result[\"entities\"])\n",
    "\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(result[\"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c82af7b3-36b3-4be3-97a7-d7182ceb4268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Medical AI Assistant...\n",
      "Loading model, please wait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 12:55:52,578 [INFO] HTTP Request: GET https://api.gradio.app/gradio-messaging/en \"HTTP/1.1 200 OK\"\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "Device set to use mps:0\n",
      "2025-04-16 12:56:16,171 [INFO] HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-04-16 12:56:16,222 [INFO] HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 12:56:17,122 [INFO] HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-04-16 12:56:17,462 [INFO] HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://5efcfa6113625c51e9.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 12:56:20,163 [INFO] HTTP Request: HEAD https://5efcfa6113625c51e9.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5efcfa6113625c51e9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "\n",
    "assistant = MedicalAIAssistant()\n",
    "\n",
    "class MedicalBot:\n",
    "    def __init__(self):\n",
    "        self.ai = assistant\n",
    "\n",
    "    def process_query(self, query, patient_id):\n",
    "        result = self.ai.process_doctor_query(query, patient_id=patient_id, user_id=\"doc001\")\n",
    "        return result[\"response\"], result[\"intent\"], result[\"terms\"]\n",
    "\n",
    "    @property\n",
    "    def ehr(self):\n",
    "        return self.ai\n",
    "\n",
    "    def handle_audio(self, audio_file, user_id, patient_id):\n",
    "        if not audio_file or not user_id.strip():\n",
    "            return \"Missing audio or user ID\", \"\", \"\"\n",
    "        if not patient_id:\n",
    "            return \"âŒ Patient ID is required\", \"\", \"\"\n",
    "        result = self.ai.process_audio_recording(audio_file, user_id)\n",
    "        return result[\"transcript\"], \", \".join(result[\"entities\"]), result[\"summary\"]\n",
    "\n",
    "    def view_patient(self, patient_id):\n",
    "        record = self.ai.view_patient_record(patient_id)\n",
    "        return json.dumps(record, indent=2)\n",
    "\n",
    "    def enroll_patient(self, name, age, gender, history):\n",
    "        pid = self.ai.enroll_new_patient(name, age, gender, history.split(\",\") if history else [])\n",
    "        return f\"New Patient ID: {pid}\", pid\n",
    "\n",
    "    def update_record(self, patient_id, note, bp, hr, temp):\n",
    "        vitals = {\n",
    "            \"blood_pressure\": bp,\n",
    "            \"heart_rate\": hr,\n",
    "            \"temperature\": temp\n",
    "        } if any([bp, hr, temp]) else None\n",
    "        result = self.ai.update_patient_record(patient_id, note=note, vitals=vitals)\n",
    "        return json.dumps(result, indent=2)\n",
    "\n",
    "    def ask_grok(self, query, patient_id):\n",
    "        if not patient_id:\n",
    "            return \"âŒ Patient ID is required.\"\n",
    "        \n",
    "        record = self.ai.view_patient_record(patient_id)\n",
    "        if \"error\" in record:\n",
    "            return f\"âŒ {record['error']}\"\n",
    "        \n",
    "        context = json.dumps(record, indent=2)\n",
    "        response = self.ai.chatbot.process_query(query, context)\n",
    "        return response\n",
    "\n",
    "bot = MedicalBot()\n",
    "\n",
    "with gr.Blocks(title=\"Medical AI Assistant\") as demo:\n",
    "    gr.Markdown(\"## ðŸ©º Medical AI Voice Assistant\")\n",
    "\n",
    "    patient_state = gr.State()\n",
    "\n",
    "    with gr.Tab(\"ðŸ—£ï¸ Audio Analysis\"):\n",
    "        user_id = gr.Textbox(label=\"User ID\")\n",
    "        audio_input = gr.Audio(label=\"Upload Audio\", type=\"filepath\")\n",
    "        transcript = gr.Textbox(label=\"Transcript\")\n",
    "        entities = gr.Textbox(label=\"Extracted Medical Entities\")\n",
    "        summary = gr.Textbox(label=\"Summary\")\n",
    "        gr.Button(\"Process Audio\").click(\n",
    "            fn=bot.handle_audio,\n",
    "            inputs=[audio_input, user_id, patient_state],\n",
    "            outputs=[transcript, entities, summary]\n",
    "        )\n",
    "\n",
    "    with gr.Tab(\"ðŸ’¬ Ask Open AI\"):\n",
    "        gr.Markdown(\"Ask medical questions based on patient record\")\n",
    "        grok_query = gr.Textbox(label=\"Your Question\")\n",
    "        grok_response = gr.Textbox(label=\"AI Response\", lines=8)\n",
    "        gr.Button(\"Ask Open AI\").click(\n",
    "            fn=bot.ask_grok,\n",
    "            inputs=[grok_query, patient_state],\n",
    "            outputs=grok_response\n",
    "        )\n",
    "\n",
    "    with gr.Tab(\"ðŸ‘¤ Patient Management\"):\n",
    "        with gr.Row():\n",
    "            name = gr.Textbox(label=\"Name\")\n",
    "            age = gr.Textbox(label=\"Age\")\n",
    "            gender = gr.Dropdown([\"Male\", \"Female\", \"Other\"], label=\"Gender\")\n",
    "            history = gr.Textbox(label=\"Medical History (comma separated)\")\n",
    "        output_pid = gr.Textbox(label=\"Patient ID\")\n",
    "        gr.Button(\"Enroll New Patient\").click(\n",
    "            fn=bot.enroll_patient,\n",
    "            inputs=[name, age, gender, history],\n",
    "            outputs=[output_pid, patient_state]\n",
    "        )\n",
    "\n",
    "    with gr.Tab(\"ðŸ“‹ Update/View Record\"):\n",
    "        pid_input = gr.Textbox(label=\"Patient ID\")\n",
    "        note = gr.Textbox(label=\"Doctor Note\")\n",
    "        bp = gr.Textbox(label=\"Blood Pressure\")\n",
    "        hr = gr.Textbox(label=\"Heart Rate\")\n",
    "        temp = gr.Textbox(label=\"Temperature\")\n",
    "        update_output = gr.Textbox(label=\"Update Result\")\n",
    "        gr.Button(\"Update Record\").click(\n",
    "            fn=bot.update_record,\n",
    "            inputs=[pid_input, note, bp, hr, temp],\n",
    "            outputs=update_output\n",
    "        )\n",
    "\n",
    "        gr.Markdown(\"### ðŸ” View Patient Record\")\n",
    "        view_output = gr.Textbox(label=\"Patient Record\", lines=10)\n",
    "        gr.Button(\"View Record\").click(\n",
    "            fn=bot.view_patient,\n",
    "            inputs=[pid_input],\n",
    "            outputs=view_output\n",
    "        )\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
